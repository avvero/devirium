это нотация, используемая в компьютерных науках для описания асимптотической сложности алгоритмов. Она выражает, как время выполнения алгоритма или его потребление памяти растет в зависимости от размера входных данных (обычно обозначаемого как n). Big(O) показывает верхнюю границу времени выполнения или использования памяти в худшем случае.

#dev #algorithm